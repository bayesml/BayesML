
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>bayesml.hiddenmarkovnormal package &#8212; BayesML</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/customize.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx-book-theme.9d8b4a8b9bb19db25eeaddc40d639ba2.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="_static/BayesML_favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="bayesml.linearregression package" href="bayesml.linearregression.html" />
    <link rel="prev" title="bayesml.gaussianmixture package" href="bayesml.gaussianmixture.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<div class="col-12 col-md-3 bd-sidebar site-navigation " id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/BayesML_logo.png" class="logo" alt="logo">
      
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="bayesml.html">
   bayesml package
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="bayesml.autoregressive.html">
     bayesml.autoregressive package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bayesml.bernoulli.html">
     bayesml.bernoulli package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bayesml.categorical.html">
     bayesml.categorical package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bayesml.contexttree.html">
     bayesml.contexttree package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bayesml.exponential.html">
     bayesml.exponential package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bayesml.gaussianmixture.html">
     bayesml.gaussianmixture package
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     bayesml.hiddenmarkovnormal package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bayesml.linearregression.html">
     bayesml.linearregression package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bayesml.metatree.html">
     bayesml.metatree package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bayesml.multivariate_normal.html">
     bayesml.multivariate_normal package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bayesml.normal.html">
     bayesml.normal package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="bayesml.poisson.html">
     bayesml.poisson package
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="developers.html">
   BayesML Developers
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
</div>

</div>


          


          
<!-- This is an invisible pixel that we watch to see if we've scrolled. -->
<div class="sbt-scroll-pixel-helper"></div>
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            <div class="topbar-left">
                
                <label class="nav-toggle-button" for="__navigation">
                    <div class="visually-hidden">Toggle navigation</div>
                    <i class="fas fa-bars"></i>
                </label>
                
            </div>
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/bayesml.hiddenmarkovnormal.rst.txt"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.rst</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/yuta-nakahara/BayesML/"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#module-bayesml.hiddenmarkovnormal">
   Module contents
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>bayesml.hiddenmarkovnormal package</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#module-bayesml.hiddenmarkovnormal">
   Module contents
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="section" id="bayesml-hiddenmarkovnormal-package">
<h1>bayesml.hiddenmarkovnormal package<a class="headerlink" href="#bayesml-hiddenmarkovnormal-package" title="Permalink to this headline">¶</a></h1>
<img alt="_images/hiddenmarkovnormal_example.png" src="_images/hiddenmarkovnormal_example.png" />
<div class="section" id="module-bayesml.hiddenmarkovnormal">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-bayesml.hiddenmarkovnormal" title="Permalink to this headline">¶</a></h2>
<p>The hidden Markov model with the Gauss-Wishart prior distribution and the Dirichlet prior distribution.</p>
<p>The stochastic data generative model is as follows:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(K \in \mathbb{N}\)</span>: number of latent classes</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{z} \in \{ 0, 1 \}^K\)</span>: a one-hot vector representing the latent class (latent variable)</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\pi} \in [0, 1]^K\)</span>: a parameter for latent classes, (<span class="math notranslate nohighlight">\(\sum_{k=1}^K \pi_k=1\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(a_{j,k} \in [0,1]\)</span> : transition probability to latent state k under latent state j</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{a}_j = [a_{j,1}, a_{j,2}, \dots , a_{j,K}]\in [0,1]^K\)</span>, a vector of the transition probability (<span class="math notranslate nohighlight">\(\sum_{k=1}^K a_{j,k}=1\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{A}=(a_{j,k})_{1\leq j,k\leq K} \in [0, 1]^{K\times K}\)</span>: a matrix of the transition probability</p></li>
<li><p><span class="math notranslate nohighlight">\(D \in \mathbb{N}\)</span>: a dimension of data</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{x} \in \mathbb{R}^D\)</span>: a data point</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k \in \mathbb{R}^D\)</span>: a parameter</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\mu} = \{ \boldsymbol{\mu}_k \}_{k=1}^K\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\Lambda}_k \in \mathbb{R}^{D\times D}\)</span> : a parameter (a positive definite matrix)</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\Lambda} = \{ \boldsymbol{\Lambda}_k \}_{k=1}^K\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(| \boldsymbol{\Lambda}_k | \in \mathbb{R}\)</span>: the determinant of <span class="math notranslate nohighlight">\(\boldsymbol{\Lambda}_k\)</span></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}p(\boldsymbol{z}_{1} | \boldsymbol{\pi}) &amp;= \mathrm{Cat}(\boldsymbol{z}_{1}|\boldsymbol{\pi}) = \prod_{k=1}^K \pi_k^{z_{1,k}},\\
p(\boldsymbol{z}_{n} |\boldsymbol{z}_{n-1} ,\boldsymbol{A}) &amp;= \prod_{k=1}^K \prod_{j=1}^K a_{j,k}^{z_{n-1,j}z_{n,k}},\\
p(\boldsymbol{x}_{n} | \boldsymbol{\mu}, \boldsymbol{\Lambda}, \boldsymbol{z}_{n}) &amp;= \prod_{k=1}^K \mathcal{N}(\boldsymbol{x}|\boldsymbol{\mu}_k,\boldsymbol{\Lambda}_k^{-1})^{z_{n,k}} \\
&amp;= \prod_{k=1}^K \left( \frac{| \boldsymbol{\Lambda}_{k} |^{1/2}}{(2\pi)^{D/2}} \exp \left\{ -\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu}_{k})^\top \boldsymbol{\Lambda}_{k} (\boldsymbol{x}-\boldsymbol{\mu}_{k}) \right\} \right)^{z_{n,k}},\end{split}\]</div>
<p>The prior distribution is as follows:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{m}_0 \in \mathbb{R}^{D}\)</span>: a hyperparameter</p></li>
<li><p><span class="math notranslate nohighlight">\(\kappa_0 \in \mathbb{R}_{&gt;0}\)</span>: a hyperparameter</p></li>
<li><p><span class="math notranslate nohighlight">\(\nu_0 \in \mathbb{R}\)</span>: a hyperparameter (<span class="math notranslate nohighlight">\(\nu_0 &gt; D-1\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{W}_0 \in \mathbb{R}^{D\times D}\)</span>: a hyperparameter (a positive definite matrix)</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\eta}_0 \in \mathbb{R}_{&gt; 0}^K\)</span>: a hyperparameter</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\zeta}_{0,j} \in \mathbb{R}_{&gt; 0}^K\)</span>: a hyperparameter</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathrm{Tr} \{ \cdot \}\)</span>: a trace of a matrix</p></li>
<li><p><span class="math notranslate nohighlight">\(\Gamma (\cdot)\)</span>: the gamma function</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}p(\boldsymbol{\mu},\boldsymbol{\Lambda},\boldsymbol{\pi},\boldsymbol{A}) &amp;= \left\{ \prod_{k=1}^K \mathcal{N}(\boldsymbol{\mu}_k|\boldsymbol{m}_0,(\kappa_0 \boldsymbol{\Lambda}_k)^{-1})\mathcal{W}(\boldsymbol{\Lambda}_k|\boldsymbol{W}_0, \nu_0) \right\} \mathrm{Dir}(\boldsymbol{\pi}|\boldsymbol{\eta}_0) \prod_{j=1}^{K}\mathrm{Dir}(\boldsymbol{a}_{j}|\boldsymbol{\zeta}_{0,j}), \\
&amp;= \Biggl[ \prod_{k=1}^K \left( \frac{\kappa_0}{2\pi} \right)^{D/2} |\boldsymbol{\Lambda}_k|^{1/2} \exp \left\{ -\frac{\kappa_0}{2}(\boldsymbol{\mu}_k -\boldsymbol{m}_0)^\top \boldsymbol{\Lambda}_k (\boldsymbol{\mu}_k - \boldsymbol{m}_0) \right\} \\
&amp;\qquad \times B(\boldsymbol{W}_0, \nu_0) | \boldsymbol{\Lambda}_k |^{(\nu_0 - D - 1) / 2} \exp \left\{ -\frac{1}{2} \mathrm{Tr} \{ \boldsymbol{W}_0^{-1} \boldsymbol{\Lambda}_k \} \right\}\biggl] \\
&amp;\qquad \times \Biggl[ \prod_{k=1}^KC(\boldsymbol{\eta}_0)\pi_k^{\eta_{0,k}-1}\biggl]\\
&amp;\qquad \times \biggl[\prod_{j=1}^KC(\boldsymbol{\zeta}_{0,j})\prod_{k=1}^K a_{j,k}^{\zeta_{0,j,k}-1}\Biggr],\\\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(B(\boldsymbol{W}_0, \nu_0)\)</span> and <span class="math notranslate nohighlight">\(C(\boldsymbol{\eta}_0)\)</span> are defined as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}B(\boldsymbol{W}_0, \nu_0) &amp;= | \boldsymbol{W}_0 |^{-\nu_0 / 2} \left( 2^{\nu_0 D / 2} \pi^{D(D-1)/4} \prod_{i=1}^D \Gamma \left( \frac{\nu_0 + 1 - i}{2} \right) \right)^{-1}, \\
C(\boldsymbol{\eta}_0) &amp;= \frac{\Gamma(\sum_{k=1}^K \eta_{0,k})}{\Gamma(\eta_{0,1})\cdots\Gamma(\eta_{0,K})},\\
C(\boldsymbol{\zeta}_{0,j}) &amp;= \frac{\Gamma(\sum_{k=1}^K \zeta_{0,j,k})}{\Gamma(\zeta_{0,j,1})\cdots\Gamma(\zeta_{0,j,K})}. \end{split}\]</div>
<p>The apporoximate posterior distribution in the <span class="math notranslate nohighlight">\(t\)</span>-th iteration of a variational Bayesian method is as follows:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{x}^n = (\boldsymbol{x}_1, \boldsymbol{x}_2, \dots , \boldsymbol{x}_n) \in \mathbb{R}^{D \times n}\)</span>: given data</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{z}^n = (\boldsymbol{z}_1, \boldsymbol{z}_2, \dots , \boldsymbol{z}_n) \in \{ 0, 1 \}^{K \times n}\)</span>: latent classes of given data</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{m}_{n,k}^{(t)} \in \mathbb{R}^{D}\)</span>: a hyperparameter</p></li>
<li><p><span class="math notranslate nohighlight">\(\kappa_{n,k}^{(t)} \in \mathbb{R}_{&gt;0}\)</span>: a hyperparameter</p></li>
<li><p><span class="math notranslate nohighlight">\(\nu_{n,k}^{(t)} \in \mathbb{R}\)</span>: a hyperparameter <span class="math notranslate nohighlight">\((\nu_n &gt; D-1)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{W}_{n,k}^{(t)} \in \mathbb{R}^{D\times D}\)</span>: a hyperparameter (a positive definite matrix)</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\eta}_n^{(t)} \in \mathbb{R}_{&gt; 0}^K\)</span>: a hyperparameter</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\zeta}_{n,j}^{(t)} \in \mathbb{R}_{&gt; 0}^K\)</span>: a hyperparameter</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}&amp;q(\boldsymbol{z}^n, \boldsymbol{\mu},\boldsymbol{\Lambda},\boldsymbol{\pi},\boldsymbol{A}) \nonumber \\
&amp;= q^{(t)}(\boldsymbol{z}^n) \left\{ \prod_{k=1}^K \mathcal{N}(\boldsymbol{\mu}_k|\boldsymbol{m}_{n,k}^{(t)},(\kappa_{n,k}^{(t)} \boldsymbol{\Lambda}_k)^{-1})\mathcal{W}(\boldsymbol{\Lambda}_k|\boldsymbol{W}_{n,k}^{(t)}, \nu_{n,k}^{(t)}) \right\} \mathrm{Dir}(\boldsymbol{\pi}|\boldsymbol{\eta}_n^{(t)})\left\{\prod_{j=1}^K\mathrm{Dir}(\boldsymbol{a}_j|\boldsymbol{\zeta}_{n,j}^{(t)})\right\}, \\
&amp;= q^{(t)}(\boldsymbol{z}^n) \Biggl[ \prod_{k=1}^K \left( \frac{\kappa_{n,k}^{(t)}}{2\pi} \right)^{D/2} |\boldsymbol{\Lambda}_k|^{1/2} \exp \left\{ -\frac{\kappa_{n,k}^{(t)}}{2}(\boldsymbol{\mu}_k -\boldsymbol{m}_{n,k}^{(t)})^\top \boldsymbol{\Lambda}_k (\boldsymbol{\mu}_k - \boldsymbol{m}_{n,k}^{(t)}) \right\} \\
&amp;\qquad \times B(\boldsymbol{W}_{n,k}^{(t)}, \nu_{n,k}^{(t)}) | \boldsymbol{\Lambda}_k |^{(\nu_{n,k}^{(t)} - D - 1) / 2} \exp \left\{ -\frac{1}{2} \mathrm{Tr} \{ ( \boldsymbol{W}_{n,k}^{(t)} )^{-1} \boldsymbol{\Lambda}_k \} \right\} \Biggr] \\
&amp;\qquad \times C(\boldsymbol{\eta}_n^{(t)})\prod_{k=1}^K \pi_k^{\eta_{n,k}^{(t)}-1}\left[\prod_{j=1}^K C(\boldsymbol{\zeta}_{n,j}^{(t)})\prod_{k=1}^K a_{j,k}^{\zeta_{n,j,k}^{(t)}-1}\right],\\\end{split}\]</div>
<p>where the updating rule of the hyperparameters is as follows.</p>
<div class="math notranslate nohighlight">
\[\begin{split}N_k^{(t)} &amp;= \sum_{i=1}^n \gamma^{(t)}_{i,k}, \\
M_{j,k}^{(t)} &amp;= \sum_{i=2}^n \xi^{(t)}_{i,j,k},\\
\bar{\boldsymbol{x}}_k^{(t)} &amp;= \frac{1}{N_k^{(t)}} \sum_{i=1}^n \gamma^{(t)}_{i,k} \boldsymbol{x}_i, \\
S_k^{(t)} &amp;= \frac{1}{N_k^{(t)}}\sum_{i=1}^n \gamma^{(t)}_{i,k} (x_i-\bar{\boldsymbol{x}}_k^{(t)})(x_i-\bar{\boldsymbol{x}}_k^{(t)})^{\top},\\
\boldsymbol{m}_{n,k}^{(t+1)} &amp;= \frac{\kappa_0\boldsymbol{\mu}_0 + N_k^{(t)} \bar{\boldsymbol{x}}_k^{(t)}}{\kappa_0 + N_k^{(t)}}, \\
\kappa_{n,k}^{(t+1)} &amp;= \kappa_0 + N_k^{(t)}, \\
(\boldsymbol{W}_{n,k}^{(t+1)})^{-1} &amp;= \boldsymbol{W}_0^{-1} + N_k^{(t)}S_k^{(t)} + \frac{\kappa_0 N_k^{(t)}}{\kappa_0 + N_k^{(t)}}(\bar{\boldsymbol{x}}_k^{(t)}-\boldsymbol{\mu}_0)(\bar{\boldsymbol{x}}_k^{(t)}-\boldsymbol{\mu}_0)^\top, \\
\nu_{n,k}^{(t+1)} &amp;= \nu_0 + N_k^{(t)},\\
\eta_{n,k}^{(t+1)} &amp;= \eta_{0,k} + \gamma^{(t)}_{1,k}, \\
\zeta_{n,j,k}^{(t+1)} &amp;= \zeta_{0,j,k}+M_{j,k}^{(t)}.\end{split}\]</div>
<p>The approximate posterior distribution of the latent variable <span class="math notranslate nohighlight">\(q^{(t+1)}(z^n)\)</span> is calculated by the forward-backward algorithm as follows.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\ln \rho_{i,k}^{(t+1)} &amp;= \frac{1}{2} \Biggl[\, \sum_{d=1}^D \psi \left( \frac{\nu_{n,k}^{(t+1)} + 1 - d}{2} \right) + D \ln 2 + \ln | \boldsymbol{W}_{n,k}^{(t+1)} | \notag \\
&amp;\qquad - D \ln (2 \pi ) - \frac{D}{\kappa_{n,k}^{(t+1)}} - \nu_{n,k}^{(t+1)} (\boldsymbol{x}_i - \boldsymbol{m}_{n,k}^{(t+1)})^\top \boldsymbol{W}_{n,k}^{(t+1)} (\boldsymbol{x}_i - \boldsymbol{m}_{n,k}^{(t+1)}) \Biggr], \\
\ln \tilde{\pi}_k^{(t+1)} &amp;= \psi (\eta_{n,k}^{(t+1)}) - \psi \left( \textstyle \sum_{k=1}^K \eta_{n,k}^{(t+1)} \right) \\
\ln \tilde{a}_{j,k}^{(t+1)} &amp;= \psi (\zeta_{n,j,k}^{(t+1)}) - \psi \left( \textstyle \sum_{k=1}^K \zeta_{n,j,k}^{(t+1)} \right) \\
\alpha^{(t+1)} (\boldsymbol{z}_i) &amp;\propto
\begin{cases}
\prod_{k=1}^{K} \left( \rho_{i,k}^{(t+1)}\right)^{z_{i,k}} \sum_{\boldsymbol{z}_{i-1}} \left[\prod_{k=1}^{K}\prod_{j=1}^{K}\left(\tilde{a}^{(t+1)}_{j,k}\right)^{z_{i-1,j}z_{i,k}}\alpha^{(t+1)}(\boldsymbol{z}_{i-1})\right] &amp; (i&gt;1)\\
\prod_{k=1}^{K}\left( \rho_{1,k}^{(t+1)} \tilde{\pi}_k^{(t+1)} \right)^{z_{1,k}} &amp; (i=1)
\end{cases} \\
\beta^{(t+1)} (\boldsymbol{z}_i) &amp;\propto
\begin{cases}
\sum_{\boldsymbol{z}_{i+1}} \left[ \prod_{k=1}^{K} \left( \rho_{i+1,k}^{(t+1)}\right)^{z_{i+1,k}} \prod_{k=1}^{K}\prod_{j=1}^{K}\left(\tilde{a}^{(t+1)}_{j,k}\right)^{z_{i,j}z_{i+1,k}}\beta^{(t+1)}(\boldsymbol{z}_{i+1})\right] &amp; (i&lt;n)\\
1 &amp; (i=n)
\end{cases} \\
q^{(t+1)}(\boldsymbol{z}_i) &amp;\propto \alpha^{(t+1)}(\boldsymbol{z}_i)\beta^{(t+1)}(\boldsymbol{z}_i) \\
\gamma^{(t+1)}_{i,k} &amp;= \sum_{\boldsymbol{z}_i} q^{(t+1)}(\boldsymbol{z}_i) z_{i,k}\\
q^{(t+1)}(\boldsymbol{z}_{i-1}, \boldsymbol{z}_{i}) &amp;\propto \alpha^{(t+1)}(\boldsymbol{z}_{i-1}) \prod_{k=1}^{K} \left( \rho_{i,k}^{(t+1)}\right)^{z_{i,k}} \prod_{k=1}^{K}\prod_{j=1}^{K}\left(\tilde{a}^{(t+1)}_{j,k}\right)^{z_{i-1,j}z_{i,k}} \beta^{(t+1)}(\boldsymbol{z}_i) \\
\xi^{(t+1)}_{i,j,k} &amp;= \sum_{\boldsymbol{z}_{i-1}} \sum_{\boldsymbol{z}_i} q^{(t+1)}(\boldsymbol{z}_{i-1}, \boldsymbol{z}_{i}) z_{i-1,j} z_{i,k}\end{split}\]</div>
<p>The approximate predictive distribution is as follows:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{x}_{n+1} \in \mathbb{R}^D\)</span>: a new data point</p></li>
<li><p><span class="math notranslate nohighlight">\((a_{\mathrm{p},j,k})_{1\leq j,k\leq K} \in [0, 1]^{K\times K}\)</span>: the parameters of the predictive transition probability of latent classes, (<span class="math notranslate nohighlight">\(\sum_{k=1}^K a_{\mathrm{p},j,k}=1\)</span>)</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{\mathrm{p},k} \in \mathbb{R}^D\)</span>: the parameter of the predictive distribution</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\Lambda}_{\mathrm{p},k} \in \mathbb{R}^{D \times D}\)</span>: the parameter of the predictive distribution (a positive definite matrix)</p></li>
<li><p><span class="math notranslate nohighlight">\(\nu_{\mathrm{p},k} \in \mathbb{R}_{&gt;0}\)</span>: the parameter of the predictive distribution</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}&amp;p(x_{n+1}|x^n) \\
&amp;\approx \sum_{k=1}^K \left( \sum_{j=1}^K \gamma_{n,j}^{(t)} a_{\mathrm{p},j,k} \right) \mathrm{St}(x_{n+1}|\boldsymbol{\mu}_{\mathrm{p},k},\boldsymbol{\Lambda}_{\mathrm{p},k}, \nu_{\mathrm{p},k}) \\
&amp;= \sum_{k=1}^K \left( \sum_{j=1}^K \gamma_{n,j}^{(t)} a_{\mathrm{p},j,k} \right)\Biggl[ \frac{\Gamma (\nu_{\mathrm{p},k} / 2 + D / 2)}{\Gamma (\nu_{\mathrm{p},k} / 2)} \frac{|\boldsymbol{\Lambda}_{\mathrm{p},k}|^{1/2}}{(\nu_{\mathrm{p},k} \pi)^{D/2}} \nonumber \\
&amp;\qquad \qquad \qquad \qquad \qquad \times \left( 1 + \frac{1}{\nu_{\mathrm{p},k}} (\boldsymbol{x}_{n+1} - \boldsymbol{\mu}_{\mathrm{p},k})^\top \boldsymbol{\Lambda}_{\mathrm{p},k} (\boldsymbol{x}_{n+1} - \boldsymbol{\mu}_{\mathrm{p},k}) \right)^{-\nu_{\mathrm{p},k}/2 - D/2} \Biggr],\end{split}\]</div>
<p>where the parameters are obtained from the hyperparameters of the predictive distribution as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}a_{\mathrm{p},j,k} &amp;= \frac{\zeta_{n,j,k}^{(t)}}{\sum_{k=1}^K \zeta_{n,j,k}^{(t)}}, \\
\boldsymbol{\mu}_{\mathrm{p},k} &amp;= \boldsymbol{m}_{n,k}^{(t)}, \\
\boldsymbol{\Lambda}_{\mathrm{p},k} &amp;= \frac{\kappa_{n,k}^{(t)} (\nu_{n,k}^{(t)} - D + 1)}{\kappa_{n,k}^{(t)} + 1} \boldsymbol{W}_{n,k}^{(t)}, \\
\nu_{\mathrm{p},k} &amp;= \nu_{n,k}^{(t)} - D + 1.\end{split}\]</div>
<dl class="py class">
<dt class="sig sig-object py" id="bayesml.hiddenmarkovnormal.GenModel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesml.hiddenmarkovnormal.</span></span><span class="sig-name descname"><span class="pre">GenModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">c_num_classes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_degree</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pi_vec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">a_mat</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mu_vecs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_mats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h_eta_vec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h_zeta_vecs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h_m_vecs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h_kappas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h_nus</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h_w_mats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bayesml.hiddenmarkovnormal.GenModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="bayesml.html#bayesml.base.Generative" title="bayesml.base.Generative"><code class="xref py py-class docutils literal notranslate"><span class="pre">bayesml.base.Generative</span></code></a></p>
<p>The stochastic data generative model and the prior distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>c_num_classes</strong><span class="classifier">int</span></dt><dd><p>a positive integer</p>
</dd>
<dt><strong>c_degree</strong><span class="classifier">int</span></dt><dd><p>a positive integer</p>
</dd>
<dt><strong>pi_vec</strong><span class="classifier">numpy.ndarray, optional</span></dt><dd><p>A vector of real numbers in <span class="math notranslate nohighlight">\([0, 1]\)</span>, 
by default [1/c_num_classes, 1/c_num_classes, … , 1/c_num_classes].
Sum of its elements must be 1.0.</p>
</dd>
<dt><strong>a_mat</strong><span class="classifier">numpy.ndarray, optional</span></dt><dd><p>A matrix of real numbers in <span class="math notranslate nohighlight">\([0, 1]\)</span>, 
by default a matrix obtained by stacking 
[1/c_num_classes, 1/c_num_classes, … , 1/c_num_classes].
Sum of the elements of each row vector must be 1.0.
If a single vector is input, will be broadcasted.</p>
</dd>
<dt><strong>mu_vecs</strong><span class="classifier">numpy.ndarray, optional</span></dt><dd><p>Vectors of real numbers, 
by default zero vectors.
If a single vector is input, will be broadcasted.</p>
</dd>
<dt><strong>lambda_mats</strong><span class="classifier">numpy.ndarray, optional</span></dt><dd><p>Positive definite symetric matrices, 
by default the identity matrices.
If a single matrix is input, it will be broadcasted.</p>
</dd>
<dt><strong>h_eta_vec</strong><span class="classifier">numpy.ndarray, optional</span></dt><dd><p>A vector of positive real numbers, 
by default [1/2, 1/2, … , 1/2]</p>
</dd>
<dt><strong>h_zeta_vecs</strong><span class="classifier">numpy.ndarray, optional</span></dt><dd><p>Vectors of positive numbers, 
by default vectors whose elements are all 1/2
If a single vector is input, will be broadcasted.</p>
</dd>
<dt><strong>h_m_vecs</strong><span class="classifier">numpy.ndarray, optional</span></dt><dd><p>Vectors of real numbers, 
by default zero vectors
If a single vector is input, will be broadcasted.</p>
</dd>
<dt><strong>h_kappas</strong><span class="classifier">float or numpy.ndarray, optional</span></dt><dd><p>Positive real numbers, 
by default [1.0, 1.0, … , 1.0].
If a single real number is input, it will be broadcasted.</p>
</dd>
<dt><strong>h_nus</strong><span class="classifier">float or numpy.ndarray, optional</span></dt><dd><p>Real numbers greater than <code class="docutils literal notranslate"><span class="pre">c_degree-1</span></code>,  
by default [c_degree, c_degree, … , c_degree]
If a single real number is input, it will be broadcasted.</p>
</dd>
<dt><strong>h_w_mats</strong><span class="classifier">numpy.ndarray, optional</span></dt><dd><p>Positive definite symetric matrices, 
by default the identity matrices.
If a single matrix is input, it will be broadcasted.</p>
</dd>
<dt><strong>seed</strong><span class="classifier">{None, int}, optional</span></dt><dd><p>A seed to initialize numpy.random.default_rng(), 
by default None</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable table autosummary">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#bayesml.hiddenmarkovnormal.GenModel.gen_params" title="bayesml.hiddenmarkovnormal.GenModel.gen_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">gen_params</span></code></a>()</p></td>
<td><p>Generate the parameter from the prior distribution.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#bayesml.hiddenmarkovnormal.GenModel.gen_sample" title="bayesml.hiddenmarkovnormal.GenModel.gen_sample"><code class="xref py py-obj docutils literal notranslate"><span class="pre">gen_sample</span></code></a>(sample_length)</p></td>
<td><p>Generate a sample from the stochastic data generative model.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#bayesml.hiddenmarkovnormal.GenModel.get_constants" title="bayesml.hiddenmarkovnormal.GenModel.get_constants"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_constants</span></code></a>()</p></td>
<td><p>Get constants of GenModel.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#bayesml.hiddenmarkovnormal.GenModel.get_h_params" title="bayesml.hiddenmarkovnormal.GenModel.get_h_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_h_params</span></code></a>()</p></td>
<td><p>Get the hyperparameters of the prior distribution.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#bayesml.hiddenmarkovnormal.GenModel.get_params" title="bayesml.hiddenmarkovnormal.GenModel.get_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_params</span></code></a>()</p></td>
<td><p>Get the parameter of the sthocastic data generative model.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_h_params</span></code>(filename)</p></td>
<td><p>Load the hyperparameters to h_params.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_params</span></code>(filename)</p></td>
<td><p>Load the parameters saved by <code class="docutils literal notranslate"><span class="pre">save_params</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">save_h_params</span></code>(filename)</p></td>
<td><p>Save the hyperparameters using python <code class="docutils literal notranslate"><span class="pre">pickle</span></code> module.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">save_params</span></code>(filename)</p></td>
<td><p>Save the parameters using python <code class="docutils literal notranslate"><span class="pre">pickle</span></code> module.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#bayesml.hiddenmarkovnormal.GenModel.save_sample" title="bayesml.hiddenmarkovnormal.GenModel.save_sample"><code class="xref py py-obj docutils literal notranslate"><span class="pre">save_sample</span></code></a>(filename, sample_length)</p></td>
<td><p>Save the generated sample as NumPy <code class="docutils literal notranslate"><span class="pre">.npz</span></code> format.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#bayesml.hiddenmarkovnormal.GenModel.set_h_params" title="bayesml.hiddenmarkovnormal.GenModel.set_h_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_h_params</span></code></a>([h_eta_vec, h_zeta_vecs, ...])</p></td>
<td><p>Set the hyperparameters of the prior distribution.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#bayesml.hiddenmarkovnormal.GenModel.set_params" title="bayesml.hiddenmarkovnormal.GenModel.set_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_params</span></code></a>([pi_vec, a_mat, mu_vecs, lambda_mats])</p></td>
<td><p>Set the parameter of the sthocastic data generative model.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#bayesml.hiddenmarkovnormal.GenModel.visualize_model" title="bayesml.hiddenmarkovnormal.GenModel.visualize_model"><code class="xref py py-obj docutils literal notranslate"><span class="pre">visualize_model</span></code></a>([sample_length])</p></td>
<td><p>Visualize the stochastic data generative model and generated samples.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="bayesml.hiddenmarkovnormal.GenModel.get_constants">
<span class="sig-name descname"><span class="pre">get_constants</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#bayesml.hiddenmarkovnormal.GenModel.get_constants" title="Permalink to this definition">¶</a></dt>
<dd><p>Get constants of GenModel.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>constants</strong><span class="classifier">dict of {str: int, numpy.ndarray}</span></dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;c_num_classes&quot;</span></code> : the value of <code class="docutils literal notranslate"><span class="pre">self.c_num_classes</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;c_degree&quot;</span></code> : the value of <code class="docutils literal notranslate"><span class="pre">self.c_degree</span></code></p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesml.hiddenmarkovnormal.GenModel.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pi_vec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">a_mat</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mu_vecs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_mats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bayesml.hiddenmarkovnormal.GenModel.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the parameter of the sthocastic data generative model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>pi_vec</strong><span class="classifier">numpy.ndarray, optional</span></dt><dd><p>A vector of real numbers in <span class="math notranslate nohighlight">\([0, 1]\)</span>, 
by default [1/c_num_classes, 1/c_num_classes, … , 1/c_num_classes].
Sum of its elements must be 1.0.</p>
</dd>
<dt><strong>a_mat</strong><span class="classifier">numpy.ndarray, optional</span></dt><dd><p>A matrix of real numbers in <span class="math notranslate nohighlight">\([0, 1]\)</span>, 
by default a matrix obtained by stacking 
[1/c_num_classes, 1/c_num_classes, … , 1/c_num_classes].
Sum of the elements of each row vector must be 1.0.
If a single vector is input, will be broadcasted.</p>
</dd>
<dt><strong>mu_vecs</strong><span class="classifier">numpy.ndarray, optional</span></dt><dd><p>Vectors of real numbers, 
by default zero vectors.
If a single vector is input, will be broadcasted.</p>
</dd>
<dt><strong>lambda_mats</strong><span class="classifier">numpy.ndarray, optional</span></dt><dd><p>Positive definite symetric matrices, 
by default the identity matrices.
If a single matrix is input, it will be broadcasted.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesml.hiddenmarkovnormal.GenModel.set_h_params">
<span class="sig-name descname"><span class="pre">set_h_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">h_eta_vec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h_zeta_vecs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h_m_vecs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h_kappas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h_nus</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h_w_mats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bayesml.hiddenmarkovnormal.GenModel.set_h_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the hyperparameters of the prior distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>h_eta_vec</strong><span class="classifier">numpy.ndarray, optional</span></dt><dd><p>A vector of positive real numbers, 
by default [1/2, 1/2, … , 1/2]</p>
</dd>
<dt><strong>h_zeta_vecs</strong><span class="classifier">numpy.ndarray, optional</span></dt><dd><p>Vectors of positive numbers, 
by default vectors whose elements are all 1/2
If a single vector is input, will be broadcasted.</p>
</dd>
<dt><strong>h_m_vecs</strong><span class="classifier">numpy.ndarray, optional</span></dt><dd><p>Vectors of real numbers, 
by default zero vectors
If a single vector is input, will be broadcasted.</p>
</dd>
<dt><strong>h_kappas</strong><span class="classifier">float or numpy.ndarray, optional</span></dt><dd><p>Positive real numbers, 
by default [1.0, 1.0, … , 1.0].
If a single real number is input, it will be broadcasted.</p>
</dd>
<dt><strong>h_nus</strong><span class="classifier">float or numpy.ndarray, optional</span></dt><dd><p>Real numbers greater than <code class="docutils literal notranslate"><span class="pre">c_degree-1</span></code>,  
by default [c_degree, c_degree, … , c_degree]
If a single real number is input, it will be broadcasted.</p>
</dd>
<dt><strong>h_w_mats</strong><span class="classifier">numpy.ndarray, optional</span></dt><dd><p>Positive definite symetric matrices, 
by default the identity matrices.
If a single matrix is input, it will be broadcasted.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesml.hiddenmarkovnormal.GenModel.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#bayesml.hiddenmarkovnormal.GenModel.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the parameter of the sthocastic data generative model.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>params</strong><span class="classifier">{str:float, numpy.ndarray}</span></dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;pi_vec&quot;</span></code> : The value of <code class="docutils literal notranslate"><span class="pre">self.pi_vec</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;a_mat&quot;</span></code> : The value of <code class="docutils literal notranslate"><span class="pre">self.a_mat</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;mu_vecs&quot;</span></code> : The value of <code class="docutils literal notranslate"><span class="pre">self.mu_vecs</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;lambda_mats&quot;</span></code> : The value of <code class="docutils literal notranslate"><span class="pre">self.lambda_mats</span></code></p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesml.hiddenmarkovnormal.GenModel.get_h_params">
<span class="sig-name descname"><span class="pre">get_h_params</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#bayesml.hiddenmarkovnormal.GenModel.get_h_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the hyperparameters of the prior distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>h_params</strong><span class="classifier">{str:float, np.ndarray}</span></dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;h_eta_vec&quot;</span></code> : The value of <code class="docutils literal notranslate"><span class="pre">self.h_eta_vec</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;h_zeta_vecs&quot;</span></code> : The value of <code class="docutils literal notranslate"><span class="pre">self.h_zeta_vecs</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;h_m_vecs&quot;</span></code> : The value of <code class="docutils literal notranslate"><span class="pre">self.h_m_vecs</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;h_kappas&quot;</span></code> : The value of <code class="docutils literal notranslate"><span class="pre">self.h_kappas</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;h_nus&quot;</span></code> : The value of <code class="docutils literal notranslate"><span class="pre">self.h_nus</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;h_w_mats&quot;</span></code> : The value of <code class="docutils literal notranslate"><span class="pre">self.h_w_mats</span></code></p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesml.hiddenmarkovnormal.GenModel.gen_params">
<span class="sig-name descname"><span class="pre">gen_params</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#bayesml.hiddenmarkovnormal.GenModel.gen_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate the parameter from the prior distribution.</p>
<p>To confirm the generated vaules, use <cite>self.get_params()</cite>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesml.hiddenmarkovnormal.GenModel.gen_sample">
<span class="sig-name descname"><span class="pre">gen_sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_length</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bayesml.hiddenmarkovnormal.GenModel.gen_sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate a sample from the stochastic data generative model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>sample_length</strong><span class="classifier">int</span></dt><dd><p>A positive integer</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>x</strong><span class="classifier">numpy ndarray</span></dt><dd><p>2-dimensional array whose shape is 
<code class="docutils literal notranslate"><span class="pre">(sample_length,c_degree)</span></code> .
Its elements are real numbers.</p>
</dd>
<dt><strong>z</strong><span class="classifier">numpy ndarray</span></dt><dd><p>2-dimensional array whose shape is 
<code class="docutils literal notranslate"><span class="pre">(sample_length,c_num_classes)</span></code> 
whose rows are one-hot vectors.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesml.hiddenmarkovnormal.GenModel.save_sample">
<span class="sig-name descname"><span class="pre">save_sample</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">filename</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_length</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bayesml.hiddenmarkovnormal.GenModel.save_sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the generated sample as NumPy <code class="docutils literal notranslate"><span class="pre">.npz</span></code> format.</p>
<p>It is saved as a NpzFile with keyword: “x”, “z”.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>filename</strong><span class="classifier">str</span></dt><dd><p>The filename to which the sample is saved.
<code class="docutils literal notranslate"><span class="pre">.npz</span></code> will be appended if it isn’t there.</p>
</dd>
<dt><strong>sample_length</strong><span class="classifier">int</span></dt><dd><p>A positive integer</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.savez_compressed.html#numpy.savez_compressed" title="(in NumPy v1.24)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.savez_compressed</span></code></a></dt><dd></dd>
</dl>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesml.hiddenmarkovnormal.GenModel.visualize_model">
<span class="sig-name descname"><span class="pre">visualize_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sample_length</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bayesml.hiddenmarkovnormal.GenModel.visualize_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Visualize the stochastic data generative model and generated samples.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>sample_length</strong><span class="classifier">int, optional</span></dt><dd><p>A positive integer, by default 100</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayesml</span> <span class="kn">import</span> <span class="n">hiddenmarkovnormal</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">hiddenmarkovnormal</span><span class="o">.</span><span class="n">GenModel</span><span class="p">(</span>
<span class="go">        c_num_classes=2,</span>
<span class="go">        c_degree=1,</span>
<span class="go">        mu_vecs=np.array([[5],[-5]]),</span>
<span class="go">        a_mat=np.array([[0.95,0.05],[0.1,0.9]]))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">visualize_model</span><span class="p">()</span>
<span class="go">pi_vec:</span>
<span class="go">[0.5 0.5]</span>
<span class="go">a_mat:</span>
<span class="go">[[0.95 0.05]</span>
<span class="go">[0.1  0.9 ]]</span>
<span class="go">mu_vecs:</span>
<span class="go">[[ 5.]</span>
<span class="go">[-5.]]</span>
<span class="go">lambda_mats:</span>
<span class="go">[[[1.]]</span>
<span class="go">[[1.]]]</span>
</pre></div>
</div>
<img alt="_images/hiddenmarkovnormal_example.png" src="_images/hiddenmarkovnormal_example.png" />
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="bayesml.hiddenmarkovnormal.LearnModel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bayesml.hiddenmarkovnormal.</span></span><span class="sig-name descname"><span class="pre">LearnModel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">c_num_classes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">c_degree</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h0_eta_vec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h0_zeta_vecs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h0_m_vecs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h0_kappas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h0_nus</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h0_w_mats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bayesml.hiddenmarkovnormal.LearnModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="bayesml.html#bayesml.base.Posterior" title="bayesml.base.Posterior"><code class="xref py py-class docutils literal notranslate"><span class="pre">bayesml.base.Posterior</span></code></a>, <a class="reference internal" href="bayesml.html#bayesml.base.PredictiveMixin" title="bayesml.base.PredictiveMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">bayesml.base.PredictiveMixin</span></code></a></p>
<p>The posterior distribution and the predictive distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>c_num_classes</strong><span class="classifier">int</span></dt><dd><p>A positive integer.</p>
</dd>
<dt><strong>c_degree</strong><span class="classifier">int</span></dt><dd><p>A positive integer.</p>
</dd>
<dt><strong>h0_eta_vec</strong><span class="classifier">numpy.ndarray, optional</span></dt><dd><p>A vector of positive real numbers, 
by default [1/2, 1/2, … , 1/2].
If a real number is input, it will be broadcasted.</p>
</dd>
<dt><strong>h0_zeta_vecs</strong><span class="classifier">numpy.ndarray, optional</span></dt><dd><p>Vectors of positive numbers, 
by default vectors whose elements are all 1.0
If a real number or a single vector is input, will be broadcasted.</p>
</dd>
<dt><strong>h0_m_vecs</strong><span class="classifier">numpy.ndarray, optional</span></dt><dd><p>Vectors of real numbers, 
by default zero vectors
If a single vector is input, will be broadcasted.</p>
</dd>
<dt><strong>h0_kappas</strong><span class="classifier">float or numpy.ndarray, optional</span></dt><dd><p>Positive real numbers, 
by default [1.0, 1.0, … , 1.0]
If a single real number is input, it will be broadcasted.</p>
</dd>
<dt><strong>h0_nus</strong><span class="classifier">float or numpy.ndarray, optional</span></dt><dd><p>Real numbers greater than c_degree-1, 
by default c_degree.
If a single real number is input, it will be broadcasted.</p>
</dd>
<dt><strong>h0_w_mats</strong><span class="classifier">numpy.ndarray, optional</span></dt><dd><p>Positive definite symetric matrices, 
by default the identity matrices
If a single matrix is input, it will be broadcasted.</p>
</dd>
<dt><strong>seed</strong><span class="classifier">{None, int}, optional</span></dt><dd><p>A seed to initialize numpy.random.default_rng(),
by default None.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Attributes</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>h0_w_mats_inv</strong><span class="classifier">numpy.ndarray</span></dt><dd><p>the inverse matrices of h0_w_mats</p>
</dd>
<dt><strong>hn_eta_vec</strong><span class="classifier">numpy.ndarray</span></dt><dd><p>A vector of positive real numbers</p>
</dd>
<dt><strong>hn_zeta_vecs</strong><span class="classifier">numpy.ndarray</span></dt><dd><p>Vectors of positive numbers</p>
</dd>
<dt><strong>hn_m_vecs</strong><span class="classifier">numpy.ndarray</span></dt><dd><p>Vectors of real numbers.</p>
</dd>
<dt><strong>hn_kappas</strong><span class="classifier">numpy.ndarray</span></dt><dd><p>Positive real numbers</p>
</dd>
<dt><strong>hn_nus</strong><span class="classifier">numpy.ndarray</span></dt><dd><p>Real numbers greater than c_degree-1.</p>
</dd>
<dt><strong>hn_w_mats</strong><span class="classifier">numpy.ndarray</span></dt><dd><p>Positive definite symetric matrices.</p>
</dd>
<dt><strong>hn_w_mats_inv</strong><span class="classifier">numpy.ndarray</span></dt><dd><p>the inverse matrices of hn_w_mats</p>
</dd>
<dt><strong>p_mu_vecs</strong><span class="classifier">numpy.ndarray</span></dt><dd><p>vectors of real numbers</p>
</dd>
<dt><strong>p_nus</strong><span class="classifier">numpy.ndarray</span></dt><dd><p>positive real numbers</p>
</dd>
<dt><strong>p_lambda_mats</strong><span class="classifier">numpy.ndarray</span></dt><dd><p>positive definite symetric matrices</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Methods</p>
<table class="longtable table autosummary">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#bayesml.hiddenmarkovnormal.LearnModel.calc_pred_dist" title="bayesml.hiddenmarkovnormal.LearnModel.calc_pred_dist"><code class="xref py py-obj docutils literal notranslate"><span class="pre">calc_pred_dist</span></code></a>()</p></td>
<td><p>Calculate the parameters of the predictive distribution.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#bayesml.hiddenmarkovnormal.LearnModel.estimate_latent_vars" title="bayesml.hiddenmarkovnormal.LearnModel.estimate_latent_vars"><code class="xref py py-obj docutils literal notranslate"><span class="pre">estimate_latent_vars</span></code></a>(x[, loss, viterbi])</p></td>
<td><p>Estimate latent variables under the given criterion.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#bayesml.hiddenmarkovnormal.LearnModel.estimate_latent_vars_and_update" title="bayesml.hiddenmarkovnormal.LearnModel.estimate_latent_vars_and_update"><code class="xref py py-obj docutils literal notranslate"><span class="pre">estimate_latent_vars_and_update</span></code></a>(x[, loss, ...])</p></td>
<td><p>Estimate latent variables and update the posterior sequentially.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#bayesml.hiddenmarkovnormal.LearnModel.estimate_params" title="bayesml.hiddenmarkovnormal.LearnModel.estimate_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">estimate_params</span></code></a>([loss])</p></td>
<td><p>Estimate the parameter under the given criterion.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#bayesml.hiddenmarkovnormal.LearnModel.get_constants" title="bayesml.hiddenmarkovnormal.LearnModel.get_constants"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_constants</span></code></a>()</p></td>
<td><p>Get constants of LearnModel.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#bayesml.hiddenmarkovnormal.LearnModel.get_h0_params" title="bayesml.hiddenmarkovnormal.LearnModel.get_h0_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_h0_params</span></code></a>()</p></td>
<td><p>Get the hyperparameters of the prior distribution.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#bayesml.hiddenmarkovnormal.LearnModel.get_hn_params" title="bayesml.hiddenmarkovnormal.LearnModel.get_hn_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_hn_params</span></code></a>()</p></td>
<td><p>Get the hyperparameters of the posterior distribution.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#bayesml.hiddenmarkovnormal.LearnModel.get_p_params" title="bayesml.hiddenmarkovnormal.LearnModel.get_p_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_p_params</span></code></a>()</p></td>
<td><p>Get the parameters of the predictive distribution.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_h0_params</span></code>(filename)</p></td>
<td><p>Load the hyperparameters to h0_params.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_hn_params</span></code>(filename)</p></td>
<td><p>Load the hyperparameters to hn_params.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#bayesml.hiddenmarkovnormal.LearnModel.make_prediction" title="bayesml.hiddenmarkovnormal.LearnModel.make_prediction"><code class="xref py py-obj docutils literal notranslate"><span class="pre">make_prediction</span></code></a>([loss])</p></td>
<td><p>Predict a new data point under the given criterion.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">overwrite_h0_params</span></code>()</p></td>
<td><p>Overwrite the initial values of the hyperparameters of the posterior distribution by the learned values.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#bayesml.hiddenmarkovnormal.LearnModel.pred_and_update" title="bayesml.hiddenmarkovnormal.LearnModel.pred_and_update"><code class="xref py py-obj docutils literal notranslate"><span class="pre">pred_and_update</span></code></a>(x[, loss, max_itr, ...])</p></td>
<td><p>Predict a new data point and update the posterior sequentially.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">reset_hn_params</span></code>()</p></td>
<td><p>Reset the hyperparameters of the posterior distribution to their initial values.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">save_h0_params</span></code>(filename)</p></td>
<td><p>Save the hyperparameters using python <code class="docutils literal notranslate"><span class="pre">pickle</span></code> module.</p></td>
</tr>
<tr class="row-even"><td><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">save_hn_params</span></code>(filename)</p></td>
<td><p>Save the hyperparameters using python <code class="docutils literal notranslate"><span class="pre">pickle</span></code> module.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#bayesml.hiddenmarkovnormal.LearnModel.set_h0_params" title="bayesml.hiddenmarkovnormal.LearnModel.set_h0_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_h0_params</span></code></a>([h0_eta_vec, h0_zeta_vecs, ...])</p></td>
<td><p>Set the hyperparameters of the prior distribution.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#bayesml.hiddenmarkovnormal.LearnModel.set_hn_params" title="bayesml.hiddenmarkovnormal.LearnModel.set_hn_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_hn_params</span></code></a>([hn_eta_vec, hn_zeta_vecs, ...])</p></td>
<td><p>Set the hyperparameter of the posterior distribution.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#bayesml.hiddenmarkovnormal.LearnModel.update_posterior" title="bayesml.hiddenmarkovnormal.LearnModel.update_posterior"><code class="xref py py-obj docutils literal notranslate"><span class="pre">update_posterior</span></code></a>(x[, max_itr, num_init, ...])</p></td>
<td><p>Update the the posterior distribution using traning data.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#bayesml.hiddenmarkovnormal.LearnModel.visualize_posterior" title="bayesml.hiddenmarkovnormal.LearnModel.visualize_posterior"><code class="xref py py-obj docutils literal notranslate"><span class="pre">visualize_posterior</span></code></a>()</p></td>
<td><p>Visualize the posterior distribution for the parameter.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="bayesml.hiddenmarkovnormal.LearnModel.get_constants">
<span class="sig-name descname"><span class="pre">get_constants</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#bayesml.hiddenmarkovnormal.LearnModel.get_constants" title="Permalink to this definition">¶</a></dt>
<dd><p>Get constants of LearnModel.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>constants</strong><span class="classifier">dict of {str: int, numpy.ndarray}</span></dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;c_num_classes&quot;</span></code> : the value of <code class="docutils literal notranslate"><span class="pre">self.c_num_classes</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;c_degree&quot;</span></code> : the value of <code class="docutils literal notranslate"><span class="pre">self.c_degree</span></code></p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesml.hiddenmarkovnormal.LearnModel.set_h0_params">
<span class="sig-name descname"><span class="pre">set_h0_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">h0_eta_vec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h0_zeta_vecs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h0_m_vecs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h0_kappas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h0_nus</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">h0_w_mats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bayesml.hiddenmarkovnormal.LearnModel.set_h0_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the hyperparameters of the prior distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>h0_eta_vec</strong><span class="classifier">numpy.ndarray, optional</span></dt><dd><p>A vector of positive real numbers, 
by default [1/2, 1/2, … , 1/2].
If a real number is input, it will be broadcasted.</p>
</dd>
<dt><strong>h0_zeta_vecs</strong><span class="classifier">numpy.ndarray, optional</span></dt><dd><p>Vectors of positive numbers, 
by default vectors whose elements are all 1.0
If a real number or a single vector is input, will be broadcasted.</p>
</dd>
<dt><strong>h0_m_vecs</strong><span class="classifier">numpy.ndarray, optional</span></dt><dd><p>Vectors of real numbers, 
by default zero vectors
If a single vector is input, will be broadcasted.</p>
</dd>
<dt><strong>h0_kappas</strong><span class="classifier">float or numpy.ndarray, optional</span></dt><dd><p>Positive real numbers, 
by default [1.0, 1.0, … , 1.0]
If a single real number is input, it will be broadcasted.</p>
</dd>
<dt><strong>h0_nus</strong><span class="classifier">float or numpy.ndarray, optional</span></dt><dd><p>Real numbers greater than c_degree-1, 
by default c_degree.
If a single real number is input, it will be broadcasted.</p>
</dd>
<dt><strong>h0_w_mats</strong><span class="classifier">numpy.ndarray, optional</span></dt><dd><p>Positive definite symetric matrices, 
by default the identity matrices
If a single matrix is input, it will be broadcasted.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesml.hiddenmarkovnormal.LearnModel.get_h0_params">
<span class="sig-name descname"><span class="pre">get_h0_params</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#bayesml.hiddenmarkovnormal.LearnModel.get_h0_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the hyperparameters of the prior distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>h0_params</strong><span class="classifier">dict of {str: numpy.ndarray}</span></dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;h0_eta_vec&quot;</span></code> : the value of <code class="docutils literal notranslate"><span class="pre">self.h0_eta_vec</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;h0_zeta_vecs&quot;</span></code> : the value of <code class="docutils literal notranslate"><span class="pre">self.h0_zeta_vecs</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;h0_m_vecs&quot;</span></code> : the value of <code class="docutils literal notranslate"><span class="pre">self.h0_m_vecs</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;h0_kappas&quot;</span></code> : the value of <code class="docutils literal notranslate"><span class="pre">self.h0_kappas</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;h0_nus&quot;</span></code> : the value of <code class="docutils literal notranslate"><span class="pre">self.h0_nus</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;h0_w_mats&quot;</span></code> : the value of <code class="docutils literal notranslate"><span class="pre">self.h0_w_mats</span></code></p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesml.hiddenmarkovnormal.LearnModel.set_hn_params">
<span class="sig-name descname"><span class="pre">set_hn_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hn_eta_vec</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hn_zeta_vecs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hn_m_vecs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hn_kappas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hn_nus</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hn_w_mats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bayesml.hiddenmarkovnormal.LearnModel.set_hn_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the hyperparameter of the posterior distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>hn_eta_vec</strong><span class="classifier">numpy.ndarray, optional</span></dt><dd><p>A vector of positive real numbers, 
by default [1/2, 1/2, … , 1/2].
If a real number is input, it will be broadcasted.</p>
</dd>
<dt><strong>hn_zeta_vecs</strong><span class="classifier">numpy.ndarray, optional</span></dt><dd><p>Vectors of positive numbers, 
by default vectors whose elements are all 1.0
If a real number or a single vector is input, will be broadcasted.</p>
</dd>
<dt><strong>hn_m_vecs</strong><span class="classifier">numpy.ndarray, optional</span></dt><dd><p>Vectors of real numbers, 
by default zero vectors
If a single vector is input, will be broadcasted.</p>
</dd>
<dt><strong>hn_kappas</strong><span class="classifier">float or numpy.ndarray, optional</span></dt><dd><p>Positive real numbers, 
by default [1.0, 1.0, … , 1.0]
If a single real number is input, it will be broadcasted.</p>
</dd>
<dt><strong>hn_nus</strong><span class="classifier">float or numpy.ndarray, optional</span></dt><dd><p>Real numbers greater than c_degree-1, 
by default c_degree.
If a single real number is input, it will be broadcasted.</p>
</dd>
<dt><strong>hn_w_mats</strong><span class="classifier">numpy.ndarray, optional</span></dt><dd><p>Positive definite symetric matrices, 
by default the identity matrices
If a single matrix is input, it will be broadcasted.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesml.hiddenmarkovnormal.LearnModel.get_hn_params">
<span class="sig-name descname"><span class="pre">get_hn_params</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#bayesml.hiddenmarkovnormal.LearnModel.get_hn_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the hyperparameters of the posterior distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>hn_params</strong><span class="classifier">dict of {str: numpy.ndarray}</span></dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;hn_eta_vec&quot;</span></code> : the value of <code class="docutils literal notranslate"><span class="pre">self.hn_eta_vec</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;hn_zeta_vecs&quot;</span></code> : the value of <code class="docutils literal notranslate"><span class="pre">self.hn_zeta_vecs</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;hn_m_vecs&quot;</span></code> : the value of <code class="docutils literal notranslate"><span class="pre">self.hn_m_vecs</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;hn_kappas&quot;</span></code> : the value of <code class="docutils literal notranslate"><span class="pre">self.hn_kappas</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;hn_nus&quot;</span></code> : the value of <code class="docutils literal notranslate"><span class="pre">self.hn_nus</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;hn_w_mats&quot;</span></code> : the value of <code class="docutils literal notranslate"><span class="pre">self.hn_w_mats</span></code></p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesml.hiddenmarkovnormal.LearnModel.update_posterior">
<span class="sig-name descname"><span class="pre">update_posterior</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_itr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tolerance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-08</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'subsampling'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bayesml.hiddenmarkovnormal.LearnModel.update_posterior" title="Permalink to this definition">¶</a></dt>
<dd><p>Update the the posterior distribution using traning data.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>x</strong><span class="classifier">numpy.ndarray</span></dt><dd><p>(sample_length,c_degree)-dimensional ndarray.
All the elements must be real number.</p>
</dd>
<dt><strong>max_itr</strong><span class="classifier">int, optional</span></dt><dd><p>maximum number of iterations, by default 100</p>
</dd>
<dt><strong>num_init</strong><span class="classifier">int, optional</span></dt><dd><p>number of initializations, by default 10</p>
</dd>
<dt><strong>tolerance</strong><span class="classifier">float, optional</span></dt><dd><p>convergence criterion of variational lower bound, by default 1.0E-8</p>
</dd>
<dt><strong>init_type</strong><span class="classifier">str, optional</span></dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">'subsampling'</span></code>: for each latent class, extract a subsample whose size is <code class="docutils literal notranslate"><span class="pre">int(np.sqrt(x.shape[0]))</span></code>, 
and use its mean and covariance matrix as an initial values of hn_m_vecs and hn_lambda_mats.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'random_responsibility'</span></code>: randomly assign responsibility to gamma_vecs</p></li>
</ul>
<p>Type of initialization, by default ‘subsampling’</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesml.hiddenmarkovnormal.LearnModel.estimate_params">
<span class="sig-name descname"><span class="pre">estimate_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'squared'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bayesml.hiddenmarkovnormal.LearnModel.estimate_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate the parameter under the given criterion.</p>
<p>Note that the criterion is applied to estimating 
<code class="docutils literal notranslate"><span class="pre">pi_vec</span></code>, <code class="docutils literal notranslate"><span class="pre">a_mat</span></code> <code class="docutils literal notranslate"><span class="pre">mu_vecs</span></code> and <code class="docutils literal notranslate"><span class="pre">lambda_mats</span></code> independently.
Therefore, a tuple of the dirichlet distribution, 
the student’s t-distributions and 
the wishart distributions will be returned when loss=”KL”</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>loss</strong><span class="classifier">str, optional</span></dt><dd><p>Loss function underlying the Bayes risk function, by default “xxx”.
This function supports “squared”, “0-1”, and “KL”.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl>
<dt><strong>Estimates</strong><span class="classifier">a tuple of {numpy ndarray, float, None, or rv_frozen}</span></dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">pi_vec_hat</span></code> : the estimate for pi_vec</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">a_mat_hat</span></code> : the estimate for a_mat</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mu_vecs_hat</span></code> : the estimate for mu_vecs</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Lambda_mats_hat</span></code> : the estimate for Lambda_mats</p></li>
</ul>
<p>The estimated values under the given loss function. 
If it is not exist, <cite>np.nan</cite> will be returned.
If the loss function is “KL”, the posterior distribution itself 
will be returned as rv_frozen object of scipy.stats.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.html#scipy.stats.rv_continuous" title="(in SciPy v1.10.1)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">scipy.stats.rv_continuous</span></code></a></dt><dd></dd>
<dt><a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_discrete.html#scipy.stats.rv_discrete" title="(in SciPy v1.10.1)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">scipy.stats.rv_discrete</span></code></a></dt><dd></dd>
</dl>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesml.hiddenmarkovnormal.LearnModel.visualize_posterior">
<span class="sig-name descname"><span class="pre">visualize_posterior</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#bayesml.hiddenmarkovnormal.LearnModel.visualize_posterior" title="Permalink to this definition">¶</a></dt>
<dd><p>Visualize the posterior distribution for the parameter.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bayesml</span> <span class="kn">import</span> <span class="n">hiddenmarkovnormal</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gen_model</span> <span class="o">=</span> <span class="n">hiddenmarkovnormal</span><span class="o">.</span><span class="n">GenModel</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">c_num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">c_degree</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">mu_vecs</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">],[</span><span class="o">-</span><span class="mi">2</span><span class="p">]]),</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">a_mat</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.95</span><span class="p">,</span><span class="mf">0.05</span><span class="p">],[</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.9</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="p">,</span><span class="n">z</span> <span class="o">=</span> <span class="n">gen_model</span><span class="o">.</span><span class="n">gen_sample</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">learn_model</span> <span class="o">=</span> <span class="n">hiddenmarkovnormal</span><span class="o">.</span><span class="n">LearnModel</span><span class="p">(</span><span class="n">c_num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">c_degree</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">learn_model</span><span class="o">.</span><span class="n">update_posterior</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">learn_model</span><span class="o">.</span><span class="n">visualize_posterior</span><span class="p">()</span>
<span class="go">hn_alpha_vec:</span>
<span class="go">[153.65657765  47.34342235]</span>
<span class="go">E[pi_vec]:</span>
<span class="go">[0.76446059 0.23553941]</span>
<span class="go">hn_zeta_vecs:</span>
<span class="go">[[147.64209251   5.51848792]</span>
<span class="go">[  5.51448518  42.3249344 ]]</span>
<span class="go">E[a_mat]</span>
<span class="go">[[0.96396927 0.03603073]</span>
<span class="go">[0.11527074 0.88472926]]</span>
<span class="go">hn_m_vecs (equivalent to E[mu_vecs]):</span>
<span class="go">[[ 1.99456861]</span>
<span class="go">[-2.15581846]]</span>
<span class="go">hn_kappas:</span>
<span class="go">[154.15657765  47.84342235]</span>
<span class="go">hn_nus:</span>
<span class="go">[154.15657765  47.84342235]</span>
<span class="go">hn_w_mats:</span>
<span class="go">[[[0.00525177]]</span>
<span class="go">[[0.02569298]]]</span>
<span class="go">E[lambda_mats]=</span>
<span class="go">[[[0.8095951 ]]</span>
<span class="go">[[1.22924015]]]</span>
</pre></div>
</div>
<img alt="_images/hiddenmarkovnormal_posterior.png" src="_images/hiddenmarkovnormal_posterior.png" />
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesml.hiddenmarkovnormal.LearnModel.get_p_params">
<span class="sig-name descname"><span class="pre">get_p_params</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#bayesml.hiddenmarkovnormal.LearnModel.get_p_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the parameters of the predictive distribution.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>p_params</strong><span class="classifier">dict of {str: numpy.ndarray}</span></dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;p_a_mat&quot;</span></code> : the value of <code class="docutils literal notranslate"><span class="pre">self.p_a_mat</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;p_mu_vecs&quot;</span></code> : the value of <code class="docutils literal notranslate"><span class="pre">self.p_mu_vecs</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;p_nus&quot;</span></code> : the value of <code class="docutils literal notranslate"><span class="pre">self.p_nus</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;p_lambda_mats&quot;</span></code> : the value of <code class="docutils literal notranslate"><span class="pre">self.p_lambda_mats</span></code></p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesml.hiddenmarkovnormal.LearnModel.calc_pred_dist">
<span class="sig-name descname"><span class="pre">calc_pred_dist</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#bayesml.hiddenmarkovnormal.LearnModel.calc_pred_dist" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the parameters of the predictive distribution.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesml.hiddenmarkovnormal.LearnModel.make_prediction">
<span class="sig-name descname"><span class="pre">make_prediction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'squared'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bayesml.hiddenmarkovnormal.LearnModel.make_prediction" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict a new data point under the given criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>loss</strong><span class="classifier">str, optional</span></dt><dd><p>Loss function underlying the Bayes risk function, by default “squared”.
This function supports “squared” and “0-1”.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>Predicted_value</strong><span class="classifier">{float, numpy.ndarray}</span></dt><dd><p>The predicted value under the given loss function.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesml.hiddenmarkovnormal.LearnModel.pred_and_update">
<span class="sig-name descname"><span class="pre">pred_and_update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'squared'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_itr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tolerance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-08</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'random_responsibility'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bayesml.hiddenmarkovnormal.LearnModel.pred_and_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict a new data point and update the posterior sequentially.</p>
<p>h0_params will be overwritten by current hn_params 
before updating hn_params by x.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>x</strong><span class="classifier">numpy.ndarray</span></dt><dd><p>It must be a <cite>c_degree</cite>-dimensional vector</p>
</dd>
<dt><strong>loss</strong><span class="classifier">str, optional</span></dt><dd><p>Loss function underlying the Bayes risk function, by default “squared”.
This function supports “squared” and “0-1”.</p>
</dd>
<dt><strong>max_itr</strong><span class="classifier">int, optional</span></dt><dd><p>maximum number of iterations, by default 100</p>
</dd>
<dt><strong>num_init</strong><span class="classifier">int, optional</span></dt><dd><p>number of initializations, by default 10</p>
</dd>
<dt><strong>tolerance</strong><span class="classifier">float, optional</span></dt><dd><p>convergence croterion of variational lower bound, by default 1.0E-8</p>
</dd>
<dt><strong>init_type</strong><span class="classifier">str, optional</span></dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">'random_responsibility'</span></code>: randomly assign responsibility to <code class="docutils literal notranslate"><span class="pre">xi_mats</span></code> and <code class="docutils literal notranslate"><span class="pre">gamma_vecs</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'subsampling'</span></code>: for each latent class, extract a subsample whose size is <code class="docutils literal notranslate"><span class="pre">int(np.sqrt(x.shape[0]))</span></code>. 
and use its mean and covariance matrix as an initial values of hn_m_vecs and hn_lambda_mats.</p></li>
</ul>
<p>Type of initialization, by default ‘random_responsibility’</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>predicted_value</strong><span class="classifier">{float, numpy.ndarray}</span></dt><dd><p>The predicted value under the given loss function.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesml.hiddenmarkovnormal.LearnModel.estimate_latent_vars">
<span class="sig-name descname"><span class="pre">estimate_latent_vars</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'0-1'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">viterbi</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bayesml.hiddenmarkovnormal.LearnModel.estimate_latent_vars" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate latent variables under the given criterion.</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">viterbi</span></code> option is <code class="docutils literal notranslate"><span class="pre">True</span></code>, this function estimates 
the latent variables maximizing the joint distribution. 
If <code class="docutils literal notranslate"><span class="pre">False</span></code>, this function independently estimates the latent 
variables at each time point.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>x</strong><span class="classifier">numpy.ndarray</span></dt><dd><p>(sample_length,c_degree)-dimensional ndarray.
All the elements must be real number.</p>
</dd>
<dt><strong>loss</strong><span class="classifier">str, optional</span></dt><dd><p>Loss function underlying the Bayes risk function, by default “0-1”.
If the <code class="docutils literal notranslate"><span class="pre">viterbi</span></code> option is <code class="docutils literal notranslate"><span class="pre">True</span></code>, this function supports only “0-1”. 
Otherwise, “0-1”, “squared”, and “KL” are supported.</p>
</dd>
<dt><strong>viterbi</strong><span class="classifier">bool, optional</span></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, this function estimates the latent variables as a sequence.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>estimates</strong><span class="classifier">numpy.ndarray</span></dt><dd><p>The estimated values under the given loss function. 
If the <code class="docutils literal notranslate"><span class="pre">viterbi</span></code> option is <code class="docutils literal notranslate"><span class="pre">False</span></code> and loss function is “KL”, 
a marginalized posterior distribution will be returned as 
a numpy.ndarray whose elements consist of occurence 
probabilities for each latent variabl.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bayesml.hiddenmarkovnormal.LearnModel.estimate_latent_vars_and_update">
<span class="sig-name descname"><span class="pre">estimate_latent_vars_and_update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'0-1'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">viterbi</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_itr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tolerance</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-08</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'subsampling'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bayesml.hiddenmarkovnormal.LearnModel.estimate_latent_vars_and_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimate latent variables and update the posterior sequentially.</p>
<p>h0_params will be overwritten by current hn_params 
before updating hn_params by x</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>x</strong><span class="classifier">numpy.ndarray</span></dt><dd><p>It must be a <cite>c_degree</cite>-dimensional vector</p>
</dd>
<dt><strong>loss</strong><span class="classifier">str, optional</span></dt><dd><p>Loss function underlying the Bayes risk function, by default “0-1”.
If the <code class="docutils literal notranslate"><span class="pre">viterbi</span></code> option is <code class="docutils literal notranslate"><span class="pre">True</span></code>, this function supports only “0-1”. 
Otherwise, “0-1”, “squared”, and “KL” are supported.</p>
</dd>
<dt><strong>viterbi</strong><span class="classifier">bool, optional</span></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, this function estimates the latent variables as a sequence.</p>
</dd>
<dt><strong>max_itr</strong><span class="classifier">int, optional</span></dt><dd><p>maximum number of iterations, by default 100</p>
</dd>
<dt><strong>num_init</strong><span class="classifier">int, optional</span></dt><dd><p>number of initializations, by default 10</p>
</dd>
<dt><strong>tolerance</strong><span class="classifier">float, optional</span></dt><dd><p>convergence croterion of variational lower bound, by default 1.0E-8</p>
</dd>
<dt><strong>init_type</strong><span class="classifier">str, optional</span></dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">'random_responsibility'</span></code>: randomly assign responsibility to <code class="docutils literal notranslate"><span class="pre">xi_mats</span></code> and <code class="docutils literal notranslate"><span class="pre">gamma_vecs</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'subsampling'</span></code>: for each latent class, extract a subsample whose size is <code class="docutils literal notranslate"><span class="pre">int(np.sqrt(x.shape[0]))</span></code>.
and use its mean and covariance matrix as an initial values of hn_m_vecs and hn_lambda_mats.</p></li>
</ul>
<p>Type of initialization, by default <code class="docutils literal notranslate"><span class="pre">'random_responsibility'</span></code></p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>estimates</strong><span class="classifier">numpy.ndarray</span></dt><dd><p>The estimated values under the given loss function. 
If the <code class="docutils literal notranslate"><span class="pre">viterbi</span></code> option is <code class="docutils literal notranslate"><span class="pre">False</span></code> and loss function is “KL”, 
a marginalized posterior distribution will be returned as 
a numpy.ndarray whose elements consist of occurence 
probabilities for each latent variabl.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>


              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="bayesml.gaussianmixture.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">bayesml.gaussianmixture package</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="bayesml.linearregression.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">bayesml.linearregression package</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By BayesML Developers<br/>
    
        &copy; Copyright 2022, BayesML Developers.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>